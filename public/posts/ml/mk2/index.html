<!DOCTYPE html>
<html
  dir="ltr"
  lang="en"
  data-theme=""
  
    class="html theme--light"
  
><head>
  <title>
    
        |
        Machine Levine Mk. II
      

    

  </title>

  
  <meta charset="utf-8" /><meta name="generator" content="Hugo 0.68.3" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <meta name="author" content="" />
  <meta
    name="description"
    content="Part 3 of the Machine Levine series, in which we explore the inner workings of Machine Levine Mk. II, the second of the Matt Bots, and primary writer of Currency Things."
  />
  
  
    
    
    <link
      rel="stylesheet"
      href="/scss/main.min.ba004976221e613a16542d07a2deca5d0377ff5e6a22dec5fcec01480ba967cf.css"
      integrity="sha256-ugBJdiIeYToWVC0Hot7KXQN3/15qIt7F/OwBSAupZ88="
      crossorigin="anonymous"
      type="text/css"
    />
  

  
  <link
    rel="stylesheet"
    href="/css/markupHighlight.min.793b323d7f2d9fbfd0b93f4d593fb5ad77b918716ed33eddea403fa87f770403.css"
    integrity="sha256-eTsyPX8tn7/QuT9NWT&#43;1rXe5GHFu0z7d6kA/qH93BAM="
    crossorigin="anonymous"
    type="text/css"
  />
  
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/fontawesome.min.7f3c2281c7f965ce3c64888aa452793252a0416909c181097f81d0a0f7d1624e.css"
    integrity="sha256-fzwigcf5Zc48ZIiKpFJ5MlKgQWkJwYEJf4HQoPfRYk4="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/solid.min.35fc032da8ede6681675d20a2f862fb9e1045c1d512d495fcf862c054daffef2.css"
    integrity="sha256-NfwDLajt5mgWddIKL4YvueEEXB1RLUlfz4YsBU2v/vI="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/regular.min.3b92357925ea7284f0c6b0378396f39f470f7842ed9702f337e667c4026bf837.css"
    integrity="sha256-O5I1eSXqcoTwxrA3g5bzn0cPeELtlwLzN&#43;ZnxAJr&#43;Dc="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link
    rel="stylesheet"
    href="/fontawesome/css/brands.min.ebb1096e1976e8cc4e2532cfa050b8f30eb13b8eb06be5cee3e38eb426b838ea.css"
    integrity="sha256-67EJbhl26MxOJTLPoFC48w6xO46wa&#43;XO4&#43;OOtCa4OOo="
    crossorigin="anonymous"
    type="text/css"
  />
  
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />

  <link rel="canonical" href="https://0xnf.github.io/posts/ml/mk2/" />

  
  
  
  
  <script
    type="text/javascript"
    src="/js/anatole-header.min.5c846cefa5ed21cceee981c70901aaf21b17b4e0b05bd777af840780918abea9.js"
    integrity="sha256-XIRs76XtIczu6YHHCQGq8hsXtOCwW9d3r4QHgJGKvqk="
    crossorigin="anonymous"
  ></script>

  
    
    
    <script
      type="text/javascript"
      src="/js/anatole-theme-switcher.min.46e991c2fcb035fc0060a792f1eb0c93ecacc9e76cd7445d6531a5cf2928686d.js"
      integrity="sha256-RumRwvywNfwAYKeS8esMk&#43;ysyeds10RdZTGlzykoaG0="
      crossorigin="anonymous"
    ></script>
  

  


  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Machine Levine Mk. II"/>
<meta name="twitter:description" content="Part 3 of the Machine Levine series, in which we explore the inner workings of Machine Levine Mk. II, the second of the Matt Bots, and primary writer of Currency Things."/>



  
  <meta property="og:title" content="Machine Levine Mk. II" />
<meta property="og:description" content="Part 3 of the Machine Levine series, in which we explore the inner workings of Machine Levine Mk. II, the second of the Matt Bots, and primary writer of Currency Things." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://0xnf.github.io/posts/ml/mk2/" />
<meta property="article:published_time" content="2018-11-03T07:22:07-07:00" />
<meta property="article:modified_time" content="2018-11-03T07:22:07-07:00" />




  
  
  
  
  <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "posts",
        "name": "Machine Levine Mk. II",
        "headline": "Machine Levine Mk. II",
        "alternativeHeadline": "",
        "description": "
      Part 3 of the Machine Levine series, in which we explore the inner workings of Machine Levine Mk. II, the second of the Matt Bots, and primary writer of Currency Things.


    ",
        "inLanguage": "en-us",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/0xnf.github.io\/posts\/ml\/mk2\/"
        },
        "author" : {
            "@type": "Person",
            "name": ""
        },
        "creator" : {
            "@type": "Person",
            "name": ""
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": ""
        },
        "copyrightHolder" : {
            "@type": "Person",
            "name": ""
        },
        "copyrightYear" : "2018",
        "dateCreated": "2018-11-03T07:22:07.00Z",
        "datePublished": "2018-11-03T07:22:07.00Z",
        "dateModified": "2018-11-03T07:22:07.00Z",
        "publisher":{
            "@type":"Organization",
            "name":  null ,
            "url": "https://0xnf.github.io/",
            "logo": {
                "@type": "ImageObject",
                "url": "https:\/\/0xnf.github.io\/favicon-32x32.png",
                "width":"32",
                "height":"32"
            }
        },
        "image": 
      [
      ]

    ,
        "url" : "https:\/\/0xnf.github.io\/posts\/ml\/mk2\/",
        "wordCount" : "2658",
        "genre" : [ 
      
      "Machine Learning"

    
      
        ,

      
      "NLP"

    
      
        ,

      
      "fast.ai"

    ],
        "keywords" : [ 
      
      "rnn"

    
      
        ,

      
      "parsing"

    
      
        ,

      
      "web scraping"

    
      
        ,

      
      "data cleanup"

    ]
    }
  </script>


</head>
<body class="body">
    <div class="wrapper">
      <aside
        
          class="wrapper__sidebar"
        
      ><div
  class="sidebar
    animated fadeInDown
  "
>
  <div class="sidebar__content">
    <div class="sidebar__introduction">
      <img
        class="sidebar__introduction-profileimage"
        src="/"
        alt="profile picture"
      />
      
        <div class="sidebar__introduction-title">
          <a href="/"></a>
        </div>
      
      <div class="sidebar__introduction-description">
        <p></p>
      </div>
    </div>
    <ul class="sidebar__list">
      
    </ul>
  </div><footer class="footer footer__sidebar">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        
        2023
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.7d3bf3472b73d9ba107b3de37d0df8fa736593a3c6638c95eaed42006bfc0193.js"
    integrity="sha256-fTvzRytz2boQez3jfQ34&#43;nNlk6PGY4yV6u1CAGv8AZM="
    crossorigin="anonymous"
  ></script></div>
</aside>
      <main
        
          class="wrapper__main"
        
      >
        <header class="header"><div
  class="
    animated fadeInDown
  "
>
  <a role="button" class="navbar-burger" data-target="navMenu" aria-label="menu" aria-expanded="false">
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
    <span aria-hidden="true" class="navbar-burger__line"></span>
  </a>
  <nav class="nav">
    <ul class="nav__list" id="navMenu">
      
      
    </ul>
    <ul class="nav__list nav__list--end">
      
      
        <li class="nav__list-item">
          <div class="themeswitch">
            <a title="Switch Theme">
              <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a>
          </div>
        </li>
      
    </ul>
  </nav>
</div>
</header>
  <div
    class="post 
      animated fadeInDown
    "
  >
    
    <div class="post__content">
      <h1>Machine Levine Mk. II</h1>
      <p>Machine Levine Mk. II is the second of the Matt Bots, and the first of which can produce any kind of coherent output. A brief description of his capabilities are described at his homepage over at <a href="http://machinelevine.winetech.com/bots/ml2,">http://machinelevine.winetech.com/bots/ml2,</a> but we&rsquo;ll deal with some deeper ideas behind his creation and operation here.</p>
<h1 id="data-collection">Data Collection</h1>
<p>Mk. II uses the same initial corpus that Mk. I uses. For more details on data collection, refer to <a href="/posts/mk2#data-collection">Mk. I&rsquo;s Data Collection</a> section.</p>
<h2 id="data-cleanup">Data Cleanup</h2>
<p>Although we use the same corpus as Mk. I and most of the same cleanup methods, Mk. II uses some different final pre-processing steps.</p>
<h3 id="tagging">Tagging</h3>
<p>It is helpful to the neural network for text to be tagged with various tokens. This helps it learn what the beginning of a sentence looks like, what a section title is, etc. During the parsing of the html documents, I transformed relevant text to include start and end tokens for various features I wanted the model to learn.</p>
<p>A Money Stuff html article is divided into an arbitrary number of sections, and each section has the following structure:</p>
<ol>
<li>A title</li>
<li>Some contents</li>
<li>Some blockquotes</li>
</ol>
<p>Sections are enclosed with <code>XBS ...section... XBE</code>, where <code>...section...</code> is the contents of the section, and XBS/XBE indicate the start and end of a section.</p>
<p>The title of a section is enclosed with <code>XCS ...title... XCE</code> where XCS/XCE indicate the start and end of a title.</p>
<p>Blockquotes, which are semantically described by the <code>&lt;blockquote&gt;</code> tags, are enclosed in XDS/XDE tags, which indicate the start and end of a blockquote.</p>
<p>The reason for tagging as opposed to leaving the html in place is that we don&rsquo;t want the model to learn how to generate html, we want it to generate Matt Levine sounding text. Any additional complexity can only make our model perform worse or in otherwise undesirable ways.</p>
<p>It is debatable whether or not an ending tag is necessary. Further revisions of the Matt Bots will explore not having ending tags except for block quotes, which are arbitrarily nested within any given section.</p>
<h3 id="mistakes">Mistakes</h3>
<p>Mk. II, like Mk. I, did not save date, title, or subtitle data for each article, and therefore, like Mk. I, has no ability to generate or use those pieces of information during training. See <a href="/posts/ml/mk1#mistakes">Mk. I&rsquo;s Mistakes Section</a> for more details.</p>
<h3 id="saving">Saving</h3>
<p>Once the data from a given article has been cleaned to an acceptable extent, it is saved into a different folder as a bunch of raw text. A single cleaned article will look like:</p>
<p><code>active-funds-and-hidden-commissions.html.txt</code>
and its contents will look like:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-txt" data-lang="txt">XAS XBS XCS Should active management be illegal? XCE Look, you know the things:The average active manager will underperform the average passive manager, because of fees.
[...]
</code></pre></div><h1 id="ml-setup">ML Setup</h1>
<p>The details of the setup can be found under <code>Matt2 - Word Level.ipynb</code>.</p>
<h2 id="wikitext-103">WikiText 103</h2>
<p>Mk. I used a character level model to construct its input and output, meanwhile Mk. II uses a word-level output. We can take advantage of this different structure and use word weights from the <a href="https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset">Wikitext103</a> project. Stephen Merity of Salesforce scanned all &ldquo;significant&rdquo; articles of English Wikipedia and pre-calculated an embedding matrix for each of the words it came across. This is a significant dataset with many millions of times more data about each word than I could have assembled using Matt&rsquo;s Money Stuff corpus alone.</p>
<p>This means we can essentially shortcut the requirement of having a ton of data about our specific domain and springboard with pre-calibrated weights for the vocabulary in our corpus. This makes our models performance better, especially for smaller data sets like ours.</p>
<h2 id="validation-and-test-sets">Validation and Test Sets</h2>
<p>The way we constructed our validation and test sets for Mk. II is identical to Mk. I. See <a href="/posts/ml/mk1#validation-and-test-sets">Mk. I&rsquo;s Validation and Test Set </a>section for more details.</p>
<p>The tl;dr is that we set aside the last 20% of our articles for our validation and test sets.</p>
<h1 id="loading-data">Loading Data</h1>
<h2 id="concatenation">Concatenation</h2>
<p>TorchText, which sits below FastAIs NLP APIs prefers to load all NLP data as a single big string, where each observation (in our case, a single article), is concatenated to the end of the previous observation.</p>
<p>Unlike Mk. I, we had the forethought to adequately tag our data this time around.</p>
<p>One of the enclosing tags I had during the parsing phase was XAS/XAE, which indicated the start and end of an article, so this is really helpful here. This way the model can potentially learn where an article starts with respect to the previous article, while also conforming to the preferred way PyTorch wants to handle this type of data.</p>
<h2 id="tokenizing">Tokenizing</h2>
<p>After concatenating, we run each Big String through <a href="https://spacy.io/models/en">Spacy</a>, which is what Fast AI prefers as its English language tokenizer.</p>
<p>Tokenization is a complicated topic on its own, but it amounts to essentially splitting a sentence into its constituent components. Usually this means splitting into individual words, but sometimes it involves sub-word components.</p>
<p>For example, 
<code>I don't want to go there!</code> turns into <code>[&quot;I&quot;, &quot;do&quot;, &quot;nt&quot;, &quot;want&quot;, &quot;to&quot;, &quot;go&quot;, &quot;there&quot;, &quot;!&quot;]</code>. Notice the <code>do</code> and <code>nt</code> tokens.</p>
<p>Jeremy&rsquo;s Tokenizer library adds some additional aspects, like lowercasing everything, adding notation for when a given token is actually upper case, and adding repetition marks, so that &lsquo;!!!&rsquo; isn&rsquo;t necessarily distinct from &lsquo;!'. In this case, <code>!!!</code> becomes something like <code>[&quot;rep&quot;, &quot;3&quot;, &quot;!&quot;]</code>, implying that the third index (<code>!</code>), should be repeated by the amount of the second index (<code>3</code>). This way the model can attempt to learn the meaning of an exclamation point once, and not have to figure out what a double exclamation point means.</p>
<p>Fast Ai is full of nice quality of life improvements like this.</p>
<h3 id="unknown-token">Unknown token</h3>
<p>We don&rsquo;t want to waste time on words that are so rare that we have no chance of learning the meaning of them, so we pick some arbitrary frequency threshold, in our case <code>2</code>, and we say that any token in our corpus that appears less than our threshold will be replaced with <code>_unk_</code>, which stands for <code>unknown</code>, and just indicates that we don&rsquo;t know the value. This helps our model by having it not waste effort on each unique unknown, and instead it uses its regular, pre-learned weights for it and moves on with its task.</p>
<h2 id="token-mappings">Token Mappings</h2>
<p>Neural Nets don&rsquo;t operate on words, they operate on numbers. I&rsquo;ll spare the details, but to facilitate this we create a mapping of our tokens to their integer representation. We have the freedom to choose what that means so we&rsquo;ll just go with a simple <code>word : index of word in token list</code> mapping. If our entire token corpus is <code>[&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;, &quot;jumped&quot;, &quot;over&quot;, &quot;the&quot;, &quot;lazy&quot;, &quot;dog&quot;]</code>, then our first mapping, String-To-Int, or <code>stoi</code>, becomes:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{
    {<span style="color:#e6db74">&#34;the&#34;</span> : <span style="color:#ae81ff">0</span>},
    {<span style="color:#e6db74">&#34;quick&#34;</span>, <span style="color:#ae81ff">1</span>},
    {<span style="color:#e6db74">&#34;brown&#34;</span>, <span style="color:#ae81ff">2</span>},
    {<span style="color:#e6db74">&#34;fox&#34;</span>, <span style="color:#ae81ff">3</span>},
    {<span style="color:#e6db74">&#34;jumps&#34;</span>, <span style="color:#ae81ff">4</span>},
    {<span style="color:#e6db74">&#34;over&#34;</span>, <span style="color:#ae81ff">5</span>},
    {<span style="color:#e6db74">&#34;lazy&#34;</span>, <span style="color:#ae81ff">6</span>},
    {<span style="color:#e6db74">&#34;dog&#34;</span>, <span style="color:#ae81ff">7</span>}
}
</code></pre></div><p>and our reverse mapping, Int-To-String, or <code>stoi</code>, becomes:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">{
    {<span style="color:#ae81ff">0</span>, <span style="color:#e6db74">&#34;the&#34;</span>},
    {<span style="color:#ae81ff">1</span>, <span style="color:#e6db74">&#34;quick&#34;</span>},
    {<span style="color:#ae81ff">2</span>, <span style="color:#e6db74">&#34;brown&#34;</span>},
    {<span style="color:#ae81ff">3</span>, <span style="color:#e6db74">&#34;fox&#34;</span>},
    {<span style="color:#ae81ff">4</span>, <span style="color:#e6db74">&#34;jumps&#34;</span>},
    {<span style="color:#ae81ff">5</span>, <span style="color:#e6db74">&#34;over&#34;</span>},
    {<span style="color:#ae81ff">6</span>, <span style="color:#e6db74">&#34;lazy&#34;</span>},
    {<span style="color:#ae81ff">7</span>, <span style="color:#e6db74">&#34;dog&#34;</span>}
}
</code></pre></div><p>These mappings are useful when converting a document into machine readable formats, and from taking output of the model and converting it into human readable formats.</p>
<h3 id="re-mapping-to-wikitext">Re-mapping to WikiText</h3>
<p>Because we&rsquo;ve opted to use WikiText&rsquo;s weights, we need to align our mappings to WikiText&rsquo;s, otherwise the embeddings will be all wrong and we&rsquo;ll get poorly trained gibberish.</p>
<p>WikiText includes a <code>itos_wt103.pkl</code> file, which contains the <code>itos</code> mapping they created. We can quickly re-map our mappings with something like</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">itos2 <span style="color:#f92672">=</span> pickle<span style="color:#f92672">.</span>load((PRE_PATH<span style="color:#f92672">/</span><span style="color:#e6db74">&#39;itos_wt103.pkl&#39;</span>)<span style="color:#f92672">.</span>open(<span style="color:#e6db74">&#39;rb&#39;</span>))
stoi2 <span style="color:#f92672">=</span> collections<span style="color:#f92672">.</span>defaultdict(<span style="color:#66d9ef">lambda</span>:<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, {v:k <span style="color:#66d9ef">for</span> k,v <span style="color:#f92672">in</span> enumerate(itos2)})
</code></pre></div><h1 id="weights">Weights</h1>
<p>We&rsquo;re using the pre-calculated weights from WikiText, so we need to load those weights into our Model before we can continue adding our custom data on top of it.</p>
<p>We&rsquo;ll make a zero-filled 2-dimensional Numpy matrix of the size <code>(vs, em_sz)</code>, where <code>vs</code> is the number of distinct tokens in our corpus (our corpus, not the generic wikitext one), and <code>em_sz</code> is the dimensionality of the embeddings for each given token. WikiText uses an embeddings size of <code>456</code>, so our 2-dimensional weight matrix will be of the shape <code>(21480, 456)</code>.</p>
<p>21480 is the number of all unique tokens that appeared more than twice in Matt&rsquo;s 571 articles.</p>
<p>Then we iterate through our re-mapped <code>itos</code> and replace the zero-filled 456-slot array for each word in our corpus that has a matching word in WikiText with the WikiText weights. This gives us a compact matrix where any extra WikiText data is discarded and only data for the words we know Matt to have written remains.</p>
<p>Of note is that for any word that doesn&rsquo;t exist, we give that word a default embedding array of <code>row_m</code>, where row_m is the <code>mean</code> of all the WikiText weights. We could choose to make it a 0-filled array, but that&rsquo;s very extreme. There is likely no word in existence that truly has zero meaning, while every word is much more likely to have a meaning closer to the average.</p>
<p>Of course, &ldquo;meaning&rdquo; here is an abstract term, and is defined along a 456-dimensional array. Even so, it makes more sense to give unknown words an average amount of &ldquo;meaning&rdquo; as opposed to &ldquo;precisely zero meaning&rdquo;.</p>
<p>Finally we assign a few sub-fields of the weights matrix to be our re-mapped and pruned weights:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">wgts[<span style="color:#e6db74">&#39;0.encoder.weight&#39;</span>] <span style="color:#f92672">=</span> T(new_w)
wgts[<span style="color:#e6db74">&#39;0.encoder_with_dropout.embed.weight&#39;</span>] <span style="color:#f92672">=</span> T(np<span style="color:#f92672">.</span>copy(new_w))
wgts[<span style="color:#e6db74">&#39;1.decoder.weight&#39;</span>] <span style="color:#f92672">=</span> T(np<span style="color:#f92672">.</span>copy(new_w))
</code></pre></div><h1 id="model-building">Model Building</h1>
<p>We&rsquo;re going to use an LSTM flavored RNN, but we don&rsquo;t have to deal with those technical details here. Using FastAI, we get a lot of stuff for free.</p>
<h2 id="hyper-parameters">Hyper Parameters</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">wd <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-7</span> <span style="color:#75715e"># Weight Decay</span>
bptt <span style="color:#f92672">=</span> <span style="color:#ae81ff">70</span> <span style="color:#75715e"># Back-Propagation-Through-Time</span>
bs <span style="color:#f92672">=</span> <span style="color:#ae81ff">52</span> <span style="color:#75715e"># Batch Size</span>
opt_fn <span style="color:#f92672">=</span> partial(optim<span style="color:#f92672">.</span>Adam, betas<span style="color:#f92672">=</span>(<span style="color:#ae81ff">0.8</span>, <span style="color:#ae81ff">0.99</span>)) <span style="color:#75715e"># Optimization Function (separate from loss. This is our strategy for navigating gradient descent.)</span>
</code></pre></div><h2 id="dataloaders">DataLoaders</h2>
<p>Using FastAIs APIs, we create a DataModel, which handles the setup of a basic neural network appropriate for our task using information it infers from the data we hand it. In our case, we&rsquo;ll ask for a LanguageLearner, and give it a list of integers, corresponding to the <code>i</code> part of our <code>stoi</code> mappings.</p>
<p>We do this for both the Train model and the Validation model:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">trn_lm <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[stoi[o] <span style="color:#66d9ef">for</span> o <span style="color:#f92672">in</span> p] <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> trainTokens])
val_lm <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[stoi[o] <span style="color:#66d9ef">for</span> o <span style="color:#f92672">in</span> p] <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> validTokens])
</code></pre></div><pre><code>trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt) #  Dataloader for 
val_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)
md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)
</code></pre><h2 id="dropout">Dropout</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">drops <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.25</span>, <span style="color:#ae81ff">0.1</span>, <span style="color:#ae81ff">0.2</span>, <span style="color:#ae81ff">0.02</span>, <span style="color:#ae81ff">0.15</span>])<span style="color:#f92672">*</span><span style="color:#ae81ff">0.7</span>
</code></pre></div><h2 id="fastai-learner-object">FastAI Learner Object</h2>
<p>One of things the FastAI Library provides on top of PyTorch is what&rsquo;s called a <code>Learner</code> object. If you provide your data in the expected format, you gain a bunch of nice properties, such as being able to do some crazy things with the learning rates really easily, like using <code>lr_find</code>, or cosine annealing, or momentum, or any number of fancy good things. It is strongly recommended to at least start with a learner object before graduating into something more low level or specific.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learner <span style="color:#f92672">=</span> md<span style="color:#f92672">.</span>get_model(opt_fn, em_sz, nh, nl, 
    dropouti<span style="color:#f92672">=</span>drops[<span style="color:#ae81ff">0</span>], dropout<span style="color:#f92672">=</span>drops[<span style="color:#ae81ff">1</span>], wdrop<span style="color:#f92672">=</span>drops[<span style="color:#ae81ff">2</span>], dropoute<span style="color:#f92672">=</span>drops[<span style="color:#ae81ff">3</span>], dropouth<span style="color:#f92672">=</span>drops[<span style="color:#ae81ff">4</span>])

learner<span style="color:#f92672">.</span>metrics <span style="color:#f92672">=</span> [accuracy] 
learner<span style="color:#f92672">.</span>freeze_to(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) 
</code></pre></div><p>Next we load the weights from earlier directly into our learner. This sets up the architecture for us.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learner<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>load_state_dict(wgts)
</code></pre></div><p>Next we set our Learning Rate. We choose 1e-3 as a good starting point. Why? Because.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span> <span style="color:#75715e"># Learning Rate</span>
lrs <span style="color:#f92672">=</span> lr <span style="color:#75715e"># Can be an array of learning rates for various &#39;layer groups&#39;. You can read more on the docs.</span>
</code></pre></div><h2 id="fitting">Fitting</h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learner<span style="color:#f92672">.</span>fit(lrs<span style="color:#f92672">/</span><span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">1</span>, wds<span style="color:#f92672">=</span>wd, use_clr<span style="color:#f92672">=</span>(<span style="color:#ae81ff">32</span>,<span style="color:#ae81ff">2</span>), cycle_len<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</code></pre></div><p>Once we&rsquo;ve done a single epoch worth&rsquo;s of training, we can try to use the <code>lr_find</code> method to find an even better rate:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learner<span style="color:#f92672">.</span>unfreeze() <span style="color:#75715e"># so that we can train every single layer. This refines the WikiText weights.</span>
learner<span style="color:#f92672">.</span>lr_find(start_lr<span style="color:#f92672">=</span>lrs<span style="color:#f92672">/</span><span style="color:#ae81ff">10</span>, end_lr<span style="color:#f92672">=</span>lrs<span style="color:#f92672">*</span><span style="color:#ae81ff">10</span>, linear<span style="color:#f92672">=</span>True)
</code></pre></div><p>Then we can plot.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learner<span style="color:#f92672">.</span>sched<span style="color:#f92672">.</span>plot()
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">learner<span style="color:#f92672">.</span>sched<span style="color:#f92672">.</span>plot_loss()
</code></pre></div><h1 id="pulling-data">Pulling Data</h1>
<p>Once we have an adequately trained model, we can pull data out of it like so:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sample_model</span>(m, s, l<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>):
    s_toks <span style="color:#f92672">=</span> Tokenizer()<span style="color:#f92672">.</span>proc_text(s)
    s_nums <span style="color:#f92672">=</span> [stoi[i] <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> s_toks]
    s_var <span style="color:#f92672">=</span> V(np<span style="color:#f92672">.</span>array(s_nums))[None]

    m[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>bs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
    m<span style="color:#f92672">.</span>eval()
    m<span style="color:#f92672">.</span>reset()

    res, <span style="color:#f92672">*</span>_ <span style="color:#f92672">=</span> m(s_var)
    <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;...&#39;</span>, end<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>)

    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(l):
        r <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>multinomial(res[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>exp(), <span style="color:#ae81ff">2</span>)
        <span style="color:#75715e">#r = torch.topk(res[-1].exp(), 2)[1]</span>
        <span style="color:#66d9ef">if</span> r<span style="color:#f92672">.</span>data[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
            r <span style="color:#f92672">=</span> r[<span style="color:#ae81ff">1</span>]
        <span style="color:#66d9ef">else</span>:
            r <span style="color:#f92672">=</span> r[<span style="color:#ae81ff">0</span>]
        
        word <span style="color:#f92672">=</span> itos[to_np(r)[<span style="color:#ae81ff">0</span>]]
        res, <span style="color:#f92672">*</span>_ <span style="color:#f92672">=</span> m(r[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>))
        <span style="color:#66d9ef">print</span>(word, end<span style="color:#f92672">=</span><span style="color:#e6db74">&#39; &#39;</span>)
    m[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>bs<span style="color:#f92672">=</span>bs
</code></pre></div><p>where <code>m</code> is the model we trained, <code>s</code> is a list of mapped tokens for it to run some predictions on, and <code>l</code> is the number of predictions we want back from it.</p>
<p>Usage looks like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">sample_model(learner<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>model, <span style="color:#e6db74">&#34;goldman sachs group&#34;</span>, <span style="color:#ae81ff">500</span>)
</code></pre></div><p>And output looks like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-txt" data-lang="txt">xbs t_up xcs things happen . t_up xce us officials ask paul singer to pay $ gapper million for the elliott management . why the treasury currency has not been cool since independence . weekend of new york fed doubles down as leverage looms . closes door eyes open for restructuring programmes as u.s . tries to lure eu restructuring . avon tips from activists to fight for rights in puerto rico . danoff bonds fund comeback after struggling to return redemption pressure . america &#39;s bond traders are being shake - ups : fed &#39;s tight bond - trading banking system hangs over .
</code></pre></div><h1 id="post-processing">Post-Processing</h1>
<p>That output is great, but is not by itself sufficient as an imitation of Money Stuff.</p>
<p>Using a post-processing script, we take the various tokens and transform them.</p>
<p><code>t_up xcs things happen .</code> becomes <code>XCS things happen.</code>, which tells us that a section title has started.</p>
<p>We also try to attach periods properly, capitalize the starts of sentences, and handle quotes. We end up with an example that looks a lot like this:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-txt" data-lang="txt">Blockchain blockchain blockchain. 

Here&#39;s a bloomberg markets story about how &#34; cryptocurrency &#34; is actually a good word for &#34; profit from early withheld ownership. &#34; the basic means of 100 percent leveraged profits last quarter were basically yes. Long - term dollar returns will be inverse, without fully 25,000 of 10,000 yen worth of stock, so you can sell a percentage of the interest rate( you get your own ). Tunick and mae&#39;s short - termism push makes them the brokerage operators for long - term value - supplying businesses, as they do well in exchange for cash and government cash. But it can be very specialized, or at least probably motivated by rigid formulas( some small private companies can buy which stocks ). There not be more in the securities market, though: &#34; lots of companies are rushing to short unthinkable&#39;s stock by requiring investment firms to buy stock, rather than returning it to shareholders. &#34;
</code></pre></div><p>Like alluded to earlier, although Mk. II can generate section titles, (and he&rsquo;s very good at that), he can&rsquo;t generate overall article titles due to errors on my part during the data cleaning phase.</p>
<p>As a solution, I take a random sampling of all the section titles he generated and assemble them into a somewhat regular sounding overall article title. From an article where the generated sections are:</p>
<ol>
<li>Blockchain blockchain blockchain.</li>
<li>Blue apron.</li>
<li>fintech.</li>
<li>Wal - mart.</li>
<li>Oh, wells fargo.</li>
<li>Ross chats.</li>
<li>philosophy.</li>
<li>What sex was settling?</li>
<li>Goldman sachs.</li>
</ol>
<p>The selected article title became: &ldquo;Goldman sachs, philosophy and ross chats&rdquo;, with a subtitle of &ldquo;also, what sex was settling?, blue apron and wal - mart&rdquo;.</p>
<h1 id="the-future">The Future</h1>
<p>Obviously I&rsquo;d like to have future Matt Bots generate their own titles properly instead of relying on post-processing.
There are also post-processing errors like incorrect quote placement, and currently there is no support for blockquotes, despite them being generated. You can see left-over blockquote token artifacts by the occasional <code>XDS/XDE</code> pair in the text.</p>
<p>I&rsquo;d also like to add new-lines and paragraph breaks back in, since his sections all run together in hard to penetrate walls of text.</p>
<p>His articles are also a little too long. I&rsquo;d like to be able to trim them by a thousand words or so.</p>
<p>Ans finally, improving his coherence is the holiest of holy grails. Pursuing higher intelligences and greater understandings of English are the foremost goal of any Matt Bot.</p>



<h3>Posts in this series</h3>
<ul>
  
    <li><a href="/posts/ml/mk2/">Machine Levine Mk. II</a></li>
  
    <li><a href="/posts/ml/mk1/">Machine Levine Mk. I</a></li>
  
    <li><a href="/posts/ml/machinelevine/">Machine Levine</a></li>
  
</ul>
</div>
    <div class="post__footer">
      
        <span><a class="category" href="/categories/machine-learning/">Machine Learning</a><a class="category" href="/categories/nlp/">NLP</a><a class="category" href="/categories/fast%2eai/">fast.ai</a></span>


      

      
        <span><a class="tag" href="/tags/rnn/">rnn</a><a class="tag" href="/tags/parsing/">parsing</a><a class="tag" href="/tags/web-scraping/">web scraping</a><a class="tag" href="/tags/data-cleanup/">data cleanup</a></span>


      
    </div>

    
  </div>

      </main>
    </div><footer class="footer footer__base">
  <ul class="footer__list">
    <li class="footer__item">
      &copy;
      
        
        2023
      
    </li>
    
  </ul>
</footer>
  
  <script
    type="text/javascript"
    src="/js/medium-zoom.min.7d3bf3472b73d9ba107b3de37d0df8fa736593a3c6638c95eaed42006bfc0193.js"
    integrity="sha256-fTvzRytz2boQez3jfQ34&#43;nNlk6PGY4yV6u1CAGv8AZM="
    crossorigin="anonymous"
  ></script></body>
</html>
