<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><title>|
Machine Levine</title><meta charset=utf-8><meta name=generator content="Hugo 0.109.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content><meta name=description content="Part 1 of the Machine Levine series, where we explore some Machine/Deep Learning. To that end, I have documented my pet problem of getting a machine to write articles like Matt Levine. What more noble goal is there than to increase the quantity of Money Stuff available to us?"><link rel=stylesheet href=/scss/main.min.5794be192d21535bdd301561e043a96b6adbad2b5c08279deff459e4661c613f.css integrity="sha256-V5S+GS0hU1vdMBVh4EOpa2rbrStcCCed7/RZ5GYcYT8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://0xnf.github.io/posts/ml/machinelevine/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Machine Levine"><meta name=twitter:description content="Part 1 of the Machine Levine series, where we explore some Machine/Deep Learning. To that end, I have documented my pet problem of getting a machine to write articles like Matt Levine. What more noble goal is there than to increase the quantity of Money Stuff available to us?"><meta property="og:title" content="Machine Levine"><meta property="og:description" content="Part 1 of the Machine Levine series, where we explore some Machine/Deep Learning. To that end, I have documented my pet problem of getting a machine to write articles like Matt Levine. What more noble goal is there than to increase the quantity of Money Stuff available to us?"><meta property="og:type" content="article"><meta property="og:url" content="https://0xnf.github.io/posts/ml/machinelevine/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-11-01T19:08:56-07:00"><meta property="article:modified_time" content="2018-11-01T19:08:56-07:00"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/mk1/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/mk2/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Machine Levine","headline":"Machine Levine","alternativeHeadline":"","description":"
      Part 1 of the Machine Levine series, where we explore some Machine\/Deep Learning. To that end, I have documented my pet problem of getting a machine to write articles like Matt Levine. What more noble goal is there than to increase the quantity of Money Stuff available to us?


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/0xnf.github.io\/posts\/ml\/machinelevine\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":{"@type":"Person","name":""},"copyrightYear":"2018","dateCreated":"2018-11-01T19:08:56.00Z","datePublished":"2018-11-01T19:08:56.00Z","dateModified":"2018-11-01T19:08:56.00Z","publisher":{"@type":"Organization","name":null,"url":"https://0xnf.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/0xnf.github.io\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/0xnf.github.io\/posts\/ml\/machinelevine\/","wordCount":"1004","genre":["Machine Learning","NLP","fast.ai"],"keywords":["rnn","parsing","web scraping","data cleanup"]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/ alt="profile picture"><div class=sidebar__introduction-title><a href=/></a></div><div class=sidebar__introduction-description><p></p></div></div><ul class=sidebar__list></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Machine Levine</h1><h1 id=the-goal>The Goal</h1><p>In order to better understand machine learning, I decided to see if I could get a neural network to write articles like Matt Levine. The original goal was to sort Spotify songs by male or female vocals, but I had been learning all of this stuff by following along with the <a href=https://course.fast.ai/>fast.ai</a> courses, and that specific use case was a bit afield of the coursework and forum discussions happening in the MOOC. Fast.ai, great as is it, has a focus on image data and NLP applications. As a compromise, I decided to learn by creating a bot that writes just like <a href=https://www.bloomberg.com/opinion/authors/ARbTQlRLRjE/matthew-s-levine>Matt Levine</a> of Bloomberg.</p><p>This new project would be much more within the course scope, and thus have a lot more material available if I ran into problems.</p><h1 id=the-finished-product>The Finished Product</h1><p>As I started playing around and getting some results with this project, I decided to turn it into something a little bigger. Now it is something akin to a bot zoo. Available at <a href=http://machinelevine.winetech.com>machinelevine.winetech.com</a>, you can see articles written by various generations of Matt Bots, as well as generate new ones written by the bot of your choosing.</p><h2 id=the-bots>The Bots</h2><p>If you&rsquo;d like to read about each bot, along with the specifics that went into them such as different tokenizations, variations in network structure, updated data techniques, etc., you find that at these links:</p><ul><li><a href=/posts/ml/mk1>Machine Levine Mk I</a></li><li><a href=/posts/ml/mk2>Machine Levine Mk II</a></li><li><a href>Machine Levine Mk III</a></li></ul><h1 id=fastai>FastAI</h1><p>The ace up the sleeve of anyone trying to teach themselves modern machine learning has got to be the free lectures by Jeremy Howard of <a href=https://www.fast.ai/>Fast.ai</a>. These courses are a top-down, practical approach, focused on getting you up and running with the tools and becoming productive as soon as possible. Once you have a taste for the various bits and functions of ML, they dive deeper and start teaching the concepts behind neural networks.</p><p>I am extremely grateful to Jeremy for creating these works and making them freely available.</p><h1 id=matt-bot>Matt Bot</h1><p>To create a bot like Matt Levine, we train a neural network on samples of <a href=https://www.bloomberg.com/view/topics/money-stuff>Money Stuff</a>, which is a daily financial newsletter the real matt writes. The final model of a given set of training data that produces some basically reasonable output is considered to be a Matt Bot.</p><h1 id=data-collection>Data Collection</h1><p>As of June 28th 2018, Matt had 571 Money Stuff articles written for Bloomberg Opinion<a href=#correction-1>*</a>. These articles form the basis of the Matt Bot input data.</p><h1 id=data-curation>Data Curation</h1><p>Just using the raw html would have included far too much noise, so the html had to undergo a lot of curation in order to be in a format appropriate for training.</p><p>The first thing to do is figure out where in the actual html does Bloomberg store the article being presented. After identifying the main div, a python script, using the excellent if slightly cumbersome BeautifulSoup to extract the article contents. Additionally, we replace every <code>&lt;a></code>, <code>&lt;strong></code>, and <code>&lt;p></code> with the interior text.</p><p>Once this is done, we extract the text-only nodes and save them to a text file. Our input has gone from looking like</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-html data-lang=html><span style=display:flex><span>&lt;<span style=color:#f92672>div</span> <span style=color:#a6e22e>class</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;body-copy fence-body&#34;</span>&gt;\n            &lt;<span style=color:#f92672>p</span>&gt;&lt;<span style=color:#f92672>strong</span>&gt;Radical truth.&lt;/<span style=color:#f92672>strong</span>&gt;&lt;/<span style=color:#f92672>p</span>&gt;&lt;<span style=color:#f92672>div</span> <span style=color:#a6e22e>class</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;hardwall&#34;</span> <span style=color:#a6e22e>data-position</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1&#34;</span>&gt;&lt;/<span style=color:#f92672>div</span>&gt;&lt;<span style=color:#f92672>div</span> <span style=color:#a6e22e>class</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softwall&#34;</span> <span style=color:#a6e22e>data-position</span><span style=color:#f92672>=</span><span style=color:#e6db74>&#34;1&#34;</span>&gt;&lt;/<span style=color:#f92672>div</span>&gt;&lt;<span style=color:#f92672>p</span>&gt;&#34;The hero comes back from this mysterious adventure with the power to bestow boons on his [...]
</span></span></code></pre></div><p>to</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-txt data-lang=txt><span style=display:flex><span>Radical truth. &#34;The hero comes back from this mysterious adventure with the power to bestow boons on his [...]
</span></span></code></pre></div><p>Each article by Matt gets its own text file.</p><h1 id=neural-network-structure>Neural Network Structure</h1><p>I used the default FastAI learner object, which dynamically selects the proper NN structure for the given input data. In this case, it&rsquo;s an LTSM-flavored RNN with an input size of <code>21,480</code>, an embedding size of <code>456</code>, some dropouts, and an output of <code>21,480</code>. This number of inputs and outputs is the cardinality of tokens, that is, all &ldquo;word-like&rdquo; things that exist in the input corpus.</p><h1 id=productionizing>Productionizing</h1><p>It does us no good to have a model available only on an expensive GPU machine, so I spent a bit of time getting everything ready to make sure the model runs on a low-budget CPU machine.</p><p>The easiest way to package a PyTorch model is to save all the weights and encodings in CPU format with the <code>.cpu()</code> call. PyTorch makes this super simple.</p><p>The more complicated aspects are making sure that some of the custom functions for sending data to and from the model are also in CPU format. This involves some fine tuning (read: guessing blindly) about what kinds of PyTorch data structures (Vectors? Tensors?) are GPU bound or not. Errors about incorrect arities of inputs can drive you berserk, but the answers are out there.</p><p>The biggest hurdle to getting all of the necessary parts onto a CPU machine is installing PyTorch, FastAI, and their dependencies on an ultra low-budget $5/month Digital Ocean server.</p><p>I spent many hours over multiple days fighting with the PyTorch installer, and specifically the Spacey tokenizer, to install in a low RAM environment.</p><p>The tl;dr is that you should increase the swapfile (<a href=https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04)>https://www.digitalocean.com/community/tutorials/how-to-add-swap-space-on-ubuntu-16-04)</a>. An extra 1GB of swap space turns the problematic install into smooth sailing.</p><h1 id=iceberg>Iceberg</h1><p>As a fun experiment, I decided to <del>steal</del> borrow the html/css from Bloomberg Opinion, so that any generated <del>Money Stuff</del> Currency Things will look just like it came from Bloomberg.</p><p>I decided to call this respectful imitation Iceberg.</p><h1 id=follow-ups>Follow Ups</h1><p>Each iteration of the Matt Bots has various tweaks and changes, not all of which are covered here. For example, MK I uses a simple character level model, Mk II uses a further curated dataset with added start/end tags for subsection titles, and Mk III uses a more refined dataset with titles and dates. Each bot is linked above, where you can read about their individual construction in more detail.</p><h3 id=footnotes>Footnotes</h3><h6 id=correction-1>Correction 1</h6><p>* Due to a quirk of how Bloomberg&rsquo;s website displays article lists, the 571 number was an accidentally too-early cut-off point that didn&rsquo;t accurately reflect how much Matt had actually written. This was corrected for Matt Bots of Mk II and greater.</p><h3>Posts in this series</h3><ul><li><a href=/posts/ml/mk2/>Machine Levine Mk. II</a></li><li><a href=/posts/ml/mk1/>Machine Levine Mk. I</a></li><li><a href=/posts/ml/machinelevine/>Machine Levine</a></li></ul></div><div class=post__footer><span><a class=category href=/categories/machine-learning/>Machine Learning</a><a class=category href=/categories/nlp/>NLP</a><a class=category href=/categories/fast%2eai/>fast.ai</a></span>
<span><a class=tag href=/tags/rnn/>rnn</a><a class=tag href=/tags/parsing/>parsing</a><a class=tag href=/tags/web-scraping/>web scraping</a><a class=tag href=/tags/data-cleanup/>data cleanup</a></span></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>