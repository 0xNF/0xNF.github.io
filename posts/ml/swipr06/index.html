<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><title>|
Swipr - Fast.ai and CNNs</title><meta charset=utf-8><meta name=generator content="Hugo 0.109.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content><meta name=description content="Part 6 of the Swipr series, wherein we finally get to the machine learning stage, and will spend all of one part before promptly moving on."><link rel=stylesheet href=/scss/main.min.5794be192d21535bdd301561e043a96b6adbad2b5c08279deff459e4661c613f.css integrity="sha256-V5S+GS0hU1vdMBVh4EOpa2rbrStcCCed7/RZ5GYcYT8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://0xnf.github.io/posts/ml/swipr06/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Swipr - Fast.ai and CNNs"><meta name=twitter:description content="Part 6 of the Swipr series, wherein we finally get to the machine learning stage, and will spend all of one part before promptly moving on."><meta property="og:title" content="Swipr - Fast.ai and CNNs"><meta property="og:description" content="Part 6 of the Swipr series, wherein we finally get to the machine learning stage, and will spend all of one part before promptly moving on."><meta property="og:type" content="article"><meta property="og:url" content="https://0xnf.github.io/posts/ml/swipr06/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-12-11T19:27:54-08:00"><meta property="article:modified_time" content="2018-12-11T19:27:54-08:00"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr01/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr02/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr03/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr04/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr05/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Swipr - Fast.ai and CNNs","headline":"Swipr - Fast.ai and CNNs","alternativeHeadline":"","description":"
      Part 6 of the Swipr series, wherein we finally get to the machine learning stage, and will spend all of one part before promptly moving on.


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/0xnf.github.io\/posts\/ml\/swipr06\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":{"@type":"Person","name":""},"copyrightYear":"2018","dateCreated":"2018-12-11T19:27:54.00Z","datePublished":"2018-12-11T19:27:54.00Z","dateModified":"2018-12-11T19:27:54.00Z","publisher":{"@type":"Organization","name":null,"url":"https://0xnf.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/0xnf.github.io\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/0xnf.github.io\/posts\/ml\/swipr06\/","wordCount":"2167","genre":["machine learning","fast.ai"],"keywords":["CNN","web scraping","data collection","architecture"]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/ alt="profile picture"><div class=sidebar__introduction-title><a href=/></a></div><div class=sidebar__introduction-description><p></p></div></div><ul class=sidebar__list></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Swipr - Fast.ai and CNNs</h1><p>Finally we can consider creating the image classification model. The jupyter notebook that was used for this process can be found <a href=https://github.com/0xNF/swipr_gh/blob/master/TrainML/swipr1.ipynb>here on GitHub</a>.</p><h1 id=dataloader>DataLoader</h1><p>The first thing the creation of a PyTorch model requires is the creation of a DataLoader, which is a neat structure for handling the data we want to go over and what their labels should be. It loads x&rsquo;s (inputs) and maps them to y&rsquo;s (outputs).</p><p>In previous DL experiments on this blog, we&rsquo;ve used text/nlp loaders, and more complicated custom loader for handling audio data, but this time since we have an image problem, we don&rsquo;t have to do anything complicated, and can use the best and easiest part fast.ai.</p><h2 id=label-csv>Label CSV</h2><p>The label CSV, in this particular use-case of an image classifier, is a file that contains the locations of our images that we want to analyze and their appropriate output classification.</p><p>We have a very simple desired output - binary classification of either YES or NO, representing a potential LIKE or PASS on Tinder.</p><p>At this point we actually have yet to properly curate these classifications for our images, having just grouped the downloaded files into a few different buckets like <code>GENERAL_NO, TOO_YOUNG</code>, etc.</p><p>We&rsquo;ll do that now.</p><p>One benefit of the SQLite file we created earlier that stored the picture locations with their respective pre-filtered status codes is that we can use that same file to construct our desired training set and their classifications on the fly.</p><h3 id=runtime-generation-of-labelscsv>Runtime Generation of Labels.csv</h3><p>First we should define what we want from our model. We downloaded a bunch of pictures, ran face detection over it, asked it for how many people were in the picture, and what their ages and genders were. Let&rsquo;s codify some rules now:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>FAIL_ON_MALE <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span> <span style=color:#75715e># We set this to true because we aren&#39;t interested in the nn learning to say yes to men.</span>
</span></span><span style=display:flex><span>MAX_PEOPLE <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#75715e># if set to zero, there is no upper limit on people in the picture. </span>
</span></span><span style=display:flex><span>MIN_PEOPLE <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#75715e># We set this to 1 because we want the NN to reject pictures of food, beaches, etc</span>
</span></span><span style=display:flex><span>MIN_AGE <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># if set to zero, there is no lower limit on the age of people in the picture. See above as to why.</span>
</span></span><span style=display:flex><span>MAX_AGE <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span> <span style=color:#75715e># if set to zero, there is no upper limit on the age of people in the picture. </span>
</span></span></code></pre></div><p>The thing we do is construct what we want a <code>NO</code> or a <code>YES</code> to be with respect to our downloaded and pre-filtered images:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>WITH_BABY <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;with_baby&#34;</span>
</span></span><span style=display:flex><span>TOO_YOUNG <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;too_young&#34;</span>
</span></span><span style=display:flex><span>TOO_OLD <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;too_old&#34;</span>
</span></span><span style=display:flex><span>PURIKURA <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;purikura&#34;</span>
</span></span><span style=display:flex><span>GENERAL_NO <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;general_no&#34;</span>
</span></span><span style=display:flex><span>OK <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;OK&#34;</span>
</span></span><span style=display:flex><span>Tinder_DUDES <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Tinder_dudes&#34;</span>
</span></span><span style=display:flex><span>Tinder_GIRLS <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Tinder_girls&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>MainDirs <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    WITH_BABY: <span style=color:#e6db74>&#34;insta/with_baby/&#34;</span>, 
</span></span><span style=display:flex><span>    TOO_YOUNG: <span style=color:#e6db74>&#34;insta/too_young/&#34;</span>, 
</span></span><span style=display:flex><span>    TOO_OLD: <span style=color:#e6db74>&#34;insta/too_old/&#34;</span>, 
</span></span><span style=display:flex><span>    PURIKURA: <span style=color:#e6db74>&#34;insta/purikura/&#34;</span>,
</span></span><span style=display:flex><span>    GENERAL_NO: <span style=color:#e6db74>&#34;insta/no/&#34;</span>,
</span></span><span style=display:flex><span>    OK: <span style=color:#e6db74>&#34;insta/ok/&#34;</span>,
</span></span><span style=display:flex><span>    Tinder_DUDES: <span style=color:#e6db74>&#34;Tinder/dudes&#34;</span>,
</span></span><span style=display:flex><span>    Tinder_GIRLS: <span style=color:#e6db74>&#34;Tinder/girls&#34;</span>,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>YesLabels <span style=color:#f92672>=</span> [OK]
</span></span><span style=display:flex><span>NoLabels <span style=color:#f92672>=</span> [Tinder_DUDES, Tinder_GIRLS, WITH_BABY, TOO_YOUNG, TOO_OLD, PURIKURA, GENERAL_NO]
</span></span></code></pre></div><p>Next we ask our SQLite file for all images that weren&rsquo;t unloadable. That is, every image that wasn&rsquo;t corrupted or something:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>Filter</span>():
</span></span><span style=display:flex><span>    conn, cursor <span style=color:#f92672>=</span> GetSql()
</span></span><span style=display:flex><span>    q <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;SELECT path, label, ages, num_people, male_present FROM pics WHERE label NOT IN (?);&#34;</span>
</span></span><span style=display:flex><span>    yeses <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    nos <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> row <span style=color:#f92672>in</span> cursor<span style=color:#f92672>.</span>execute(q, (FAIL_INVALID_INT, )):
</span></span><span style=display:flex><span>        p <span style=color:#f92672>=</span> row[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> _Filter(row):
</span></span><span style=display:flex><span>            yeses<span style=color:#f92672>.</span>append(p)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            nos<span style=color:#f92672>.</span>append(p)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> yeses, nos
</span></span></code></pre></div><p>Then we put our pre-filter to work assembling our <code>YesLabels</code>. Just because the picture was from the <code>OK</code> path of our Instagram directories doesn&rsquo;t mean we want to blindly accept it. Remember, no beaches and no food:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_Filter</span>(row, verbose<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Get the path of our consideration</span>
</span></span><span style=display:flex><span>    p <span style=color:#f92672>=</span> row[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check that path isn&#39;t in our unconditional &#39;no&#39; set</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> nopath <span style=color:#f92672>in</span> [MainDirs[x] <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> NoLabels]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> p<span style=color:#f92672>.</span>startswith(nopath):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check that the label doesn&#39;t immediately disqualify from consideration</span>
</span></span><span style=display:flex><span>    label <span style=color:#f92672>=</span> row[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> label <span style=color:#f92672>in</span> [FAIL_NOFACE_INT, FAIL_ROTATEDNOCHANGED_INT]:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># check number of people</span>
</span></span><span style=display:flex><span>    num_people <span style=color:#f92672>=</span> row[<span style=color:#ae81ff>3</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> MAX_PEOPLE <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> num_people <span style=color:#f92672>&gt;</span> MAX_PEOPLE:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check that there are no males</span>
</span></span><span style=display:flex><span>    male_present <span style=color:#f92672>=</span> row[<span style=color:#ae81ff>4</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> FAIL_ON_MALE <span style=color:#f92672>and</span> male_present:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Check ages</span>
</span></span><span style=display:flex><span>    age_str <span style=color:#f92672>=</span> row[<span style=color:#ae81ff>2</span>]<span style=color:#f92672>.</span>split(<span style=color:#e6db74>&#39;,&#39;</span>)
</span></span><span style=display:flex><span>    ages <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> age_str:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>            a <span style=color:#f92672>=</span> int(x)
</span></span><span style=display:flex><span>            ages<span style=color:#f92672>.</span>append(a)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>except</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> MIN_AGE <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        min_passed <span style=color:#f92672>=</span> all([x <span style=color:#f92672>&gt;=</span> MIN_AGE <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> ages])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> min_passed:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> MAX_AGE <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:     
</span></span><span style=display:flex><span>        max_passed <span style=color:#f92672>=</span> all([x <span style=color:#f92672>&lt;=</span> MAX_AGE <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> ages])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> max_passed:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>True</span>
</span></span></code></pre></div><p>The output of all this is a list of image paths separated into legitimate <code>YES</code> images, and the rest of them are <code>NO</code>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>y,n <span style=color:#f92672>=</span> Filter()
</span></span><span style=display:flex><span>len(n)
</span></span><span style=display:flex><span><span style=color:#ae81ff>201573</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>len(yeses)
</span></span><span style=display:flex><span><span style=color:#ae81ff>47777</span>
</span></span></code></pre></div><p>It&rsquo;s a heavily skewed dataset, but such is life on Instagram:
Roughly 50,000 images from our 300,000 collection are ok to learn what a Tinder LIKE should be, and the rest are all what a Tinder PASS should be.</p><p>Recall that passes will include the whole shebang: Pictures of dogs, beaches, food, men, and women who just aren&rsquo;t our type.</p><p>We can now assemble our labels.csv on the fly using these modular creation methods:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>MakeCSV</span>(cname, yeses, nos):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> open(PATH<span style=color:#f92672>/</span>cname, <span style=color:#e6db74>&#39;w&#39;</span>, newline<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;&#39;</span>, encoding<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;utf8&#34;</span>) <span style=color:#66d9ef>as</span> csvfile:
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        resolved <span style=color:#f92672>=</span> PATH<span style=color:#f92672>.</span>resolve()
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># get a csv writer with some defaults</span>
</span></span><span style=display:flex><span>        cwriter <span style=color:#f92672>=</span> csv<span style=color:#f92672>.</span>writer(csvfile, delimiter<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;,&#39;</span>, quotechar<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;|&#39;</span>, quoting<span style=color:#f92672>=</span>csv<span style=color:#f92672>.</span>QUOTE_MINIMAL)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># Write the header</span>
</span></span><span style=display:flex><span>        cwriter<span style=color:#f92672>.</span>writerow([<span style=color:#e6db74>&#34;path&#34;</span>, <span style=color:#e6db74>&#34;label&#34;</span>])
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#75715e># write the nos</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> nos:
</span></span><span style=display:flex><span>            cwriter<span style=color:#f92672>.</span>writerow([os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(resolved, p), <span style=color:#e6db74>&#34;NO&#34;</span>])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>        <span style=color:#75715e># write the yesses</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> yeses:
</span></span><span style=display:flex><span>            cwriter<span style=color:#f92672>.</span>writerow([os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(resolved, p), <span style=color:#e6db74>&#34;YES&#34;</span>])
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> PATH<span style=color:#f92672>/</span>cname
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>label_csv <span style=color:#f92672>=</span> MakeCSV(<span style=color:#e6db74>&#34;labels.csv&#34;</span>, y, n)
</span></span></code></pre></div><h2 id=validation-set>Validation Set</h2><p>fast.ai offers an easy way to divide a set of labels into a training set and a validation set via the <code>get_cv_indexes</code> method, which we will use here:</p><pre tabindex=0><code>n = len(list(open(label_csv)))-1
val_idxs = get_cv_idxs(n)
</code></pre><p>The method randomly chooses some percentage of the total dataset to become the validation set. In our case, we got about 50,000 images for the validation set:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>len(val_idxs)
</span></span><span style=display:flex><span><span style=color:#ae81ff>50991</span>
</span></span></code></pre></div><h2 id=architecture>Architecture</h2><p>It&rsquo;s now time to make perhaps the most major choice regarding our model yet. What will our architecture be? Should we build something from scratch or use a pre-existing arch? What should our initial weights be?</p><p>Although these are heavy questions, we will make our decision with somewhat less gravitas. As the adage for deep learning projects goes, if you have an image classification problem, throw ResNet at it and see if it works.</p><p>For no reason in particular, instead of ResNet, we&rsquo;ll use FAIR&rsquo;s <code>ResNext50</code>. Why? Because. It&rsquo;s like ResNet, but not. It comes pre-installed with the fast.ai libraries and is pre-trained on weights from ImageNet.</p><p>This is particularly useful to us because ImageNet is really good for real-world photographs, and shouldn&rsquo;t require much additional training. That is, we don&rsquo;t have to spend time teaching it what a &ldquo;human&rdquo; is, and can instead essentially jump right into teaching it what is worth swiping left or right on.</p><h2 id=size>Size</h2><p>One of the image-specific settings we need to set is what the dimension of the images we&rsquo;re going to hand into the model should be. Although named needlessly similar to the BatchSize parameter, they aren&rsquo;t related concepts.</p><p>Images need to be square for the current state-of-the-art convnet algos to do their magic, and they can&rsquo;t be especially large squares at that.</p><p>We&rsquo;ll be taking a lot of cues from Jeremey&rsquo;s fast.ai introductory lesson about Dogs vs Cats since it matches our problem of binary image classification well.</p><p>There he chose a dimension size of <code>224 x 224</code>. Images beyond that size will be resized and cropped if necessary, and images below that size will be padded out to make up for it.</p><h2 id=batch-size>Batch Size</h2><p>This is one of our most magical hyperparameters, second only in importance to the mythical learning rate.</p><p>As a quick overview, the batch size is the number of observations (in our case, discrete images) that will be loaded onto the GPU at once. These images are all operated upon together and used to discern the gradient to descend. This is the data the <code>gradient</code> part of <code>gradient descent</code> is calculated from.</p><p>In theory, the bigger a batch a size the more granular and accurate the resulting gradient is, but it is neither practical (due to GPU memory limitations) nor even necessarily useful to try to maximize this batch size parameter. In some cases it may even be harmful, contributing in part to why the batch size is considered magical.</p><p>If you&rsquo;re following along with Jeremy&rsquo;s courses, you may hear him refer to a <code>minibatch</code> - that refers to a <code>BatchSize</code>-sized list of data being handed to the neural net for gradient calculations. In essence:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-C data-lang=C><span style=display:flex><span>Image[] minibatch <span style=color:#f92672>=</span> new Image[BatchSize];
</span></span></code></pre></div><p>Our batch size here, like in the Cats vs Dogs lesson, will be <code>24</code>, but you are encouraged to play around and measure your results.</p><h2 id=transforms>Transforms</h2><p>We keep saying fast.ai has splendid support for you if your use-case involves going over images and our next task is a small testament to that.</p><p>fast.ai supports the notion of data augmentation during training, which is a way to avoid overfitting by slightly adjusting the data handed in while attempting to keep the meaning of the data unchanged. In our case, we want to run some transformations over our training images:</p><pre tabindex=0><code>tfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)
</code></pre><p>This function, supplied by the fast.ai library, produces a list of other functions which will transform each image in various ways.</p><p>Basically, we&rsquo;ll flip the images horizontally and zoom in up to an additional 10%.</p><p>The idea behind this step is that a picture of, for example, a cat, is a cat even if it&rsquo;s zoomed in, or even if it&rsquo;s facing the other direction. Cat&rsquo;s are not unidirectional objects so we want our NN to avoid fitting to that particular aspect.</p><p>The same idea applies for Tinder pictures.</p><h2 id=back-to-the-dataloader>Back to the DataLoader</h2><p>We&rsquo;re going to combine everything from above into a custom dataloader creation function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_data</span>(size, batchSize):
</span></span><span style=display:flex><span>    tfms <span style=color:#f92672>=</span> tfms_from_model(arch, size, aug_tfms<span style=color:#f92672>=</span>transforms_side_on, max_zoom<span style=color:#f92672>=</span><span style=color:#ae81ff>1.1</span>)
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> ImageClassifierData<span style=color:#f92672>.</span>from_csv(PATH, PATH, label_csv, val_idxs<span style=color:#f92672>=</span>val_idxs, tfms<span style=color:#f92672>=</span>tfms, bs<span style=color:#f92672>=</span>bs)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> data <span style=color:#66d9ef>if</span> size <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>300</span> <span style=color:#66d9ef>else</span> data<span style=color:#f92672>.</span>resize(<span style=color:#ae81ff>340</span>, <span style=color:#e6db74>&#34;tmp&#34;</span>)
</span></span></code></pre></div><p>This is taken straight from the <code>Dogs vs Cats</code> lesson. It resizes each picture in the dataset and stores everything into <code>tmp</code>.</p><p>Our dataloader can be instantiated like so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data <span style=color:#f92672>=</span> get_data(sz, bs)
</span></span></code></pre></div><p>The paths are for the train and validation sets, because images are expected to be relative to the current working directory. We pass in the same path for both fields, along with the <code>val_idxs</code>, <code>tfms</code>, and <code>bs</code> from earlier.</p><p>We can see the relative sizes of our datasets like so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>len(data<span style=color:#f92672>.</span>trn_ds), len(data<span style=color:#f92672>.</span>val_ds)
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>203966</span>, <span style=color:#ae81ff>50991</span>)
</span></span></code></pre></div><p>We can check our classes:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>len(data<span style=color:#f92672>.</span>classes), data<span style=color:#f92672>.</span>classes
</span></span><span style=display:flex><span>(<span style=color:#ae81ff>2</span>, [<span style=color:#e6db74>&#39;NO&#39;</span>, <span style=color:#e6db74>&#39;YES&#39;</span>])
</span></span></code></pre></div><p>Notably this means that index <code>0</code> is <code>NO</code> and index <code>1</code> is <code>YES</code>. This is good information for later.</p><h1 id=learner-object>Learner Object</h1><p>The last step of our model creation odyssey is to instantiate a learner object and call <code>fit</code> on it:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn <span style=color:#f92672>=</span> ConvLearner<span style=color:#f92672>.</span>pretrained(arch, data, precompute<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span></code></pre></div><p>We ask for a pretrained learner because we want to use the weights from ResNext50, and we want <code>precompute=False</code> because we&rsquo;re not trying to fine-tune the last layers, we want to spend some effort really substantially updating all the intermediate weights.</p><p>Of course like every other time we start this process with the learning rate finder:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn<span style=color:#f92672>.</span>lr_find()
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>sched<span style=color:#f92672>.</span>plot_lr()
</span></span></code></pre></div><p>The usual graphs:</p><p><img src=/ml/swipr/lr_iter.png alt=iterations> <img src=/ml/swipr/lr_lr.png alt=lr_rate></p><p>We want to find the learning ate that seems most likely to give us the greatest ability to descend, while also leading us to the widest area of our data, spatially speaking. From the graph, although a learning rate of <code>1e^-0.5</code> leads to a good curve, <code>1e-4</code> leads us to a similar depth while giving us a wider valley.</p><p>We&rsquo;ll choose <code>1e-4</code> first and see where we go from there:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn<span style=color:#f92672>.</span>fit(<span style=color:#ae81ff>1e-4</span>, <span style=color:#ae81ff>3</span>, cycle_len<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, cycle_mult<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><p>After a very long time, we got the following results:</p><table><thead><tr><th>epoch         </th><th>trn_loss         </th><th>val_loss         </th><th>accuracy</th></tr></thead><tbody><tr><td>0</td><td>0.399513</td><td>0.392782</td><td>0.819912</td></tr><tr><td>1</td><td>0.403108</td><td>0.390016</td><td>0.822238</td></tr><tr><td>2</td><td>0.393089</td><td>0.39024</td><td>0.822318</td></tr><tr><td>3</td><td>0.388752</td><td>0.38785</td><td>0.824343</td></tr><tr><td>4</td><td>0.399516</td><td>0.384942</td><td>0.825968</td></tr></tbody></table><p>ImageNet weights are already good, so we start right on off with a 0.38 validation loss.</p><p>Next, we tried the following:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>learn<span style=color:#f92672>.</span>set_data(get_data(<span style=color:#ae81ff>229</span>,bs))
</span></span><span style=display:flex><span>learn<span style=color:#f92672>.</span>freeze()
</span></span></code></pre></div><p>Primarily we freeze everything but the last layer and see if we get better results. We also try with a slightly higher learning rate of <code>1e-3</code>.</p><p>We end up with the following:</p><pre tabindex=0><code>learn.fit(1e-3, 3, cycle_len=1, cycle_mult=2)
</code></pre><table><thead><tr><th>epoch         </th><th>trn_loss         </th><th>val_loss         </th><th>accuracy</th></tr></thead><tbody><tr><td>0</td><td>0.372923</td><td>0.365708</td><td>0.832845</td></tr><tr><td>1</td><td>0.359612</td><td>0.364647</td><td>0.832565</td></tr><tr><td>2</td><td>0.382324</td><td>0.368091</td><td>0.832244</td></tr></tbody></table><p>We tried a number of other possibilities, involving both starting at different learning rates, or using the same object and changing the learning rate, and also resizing</p><p>It looks like we&rsquo;re topping out around a 0.36 validation loss meaning a roughly 83% accuracy.</p><p>Training these variants totaled around 10 hours.</p><h1 id=other-considerations>Other Considerations</h1><p>Potential other model architectures included integrating the <code>InsightFace</code> and <code>DFace</code> steps, creating something from scratch, or otherwise better integrating the Tinder pictures, because we just labeled them all as <code>NO</code> by default.</p><p>Needless to say, we went the lazy route, because an 83% accuracy is already pretty good.</p><h1 id=moving-on>Moving on</h1><p>In the next post we&rsquo;ll discuss moving the model to our CPU-only server and constructing a small framework to interact with it.</p><p><a href=/posts/ml/swipr07>Next Section - Swipr Script Service</a></p><h3>Posts in this series</h3><ul><li><a href=/posts/ml/swipr10/>Swipr - Server</a></li><li><a href=/posts/ml/swipr09/>Swipr - Datastore</a></li><li><a href=/posts/ml/swipr08/>Swipr - LibSwipr</a></li><li><a href=/posts/ml/swipr07/>Swipr - Swipr Script Service</a></li><li><a href=/posts/ml/swipr06/>Swipr - Fast.ai and CNNs</a></li><li><a href=/posts/ml/swipr05/>Swipr - Data Cleaning</a></li><li><a href=/posts/ml/swipr04/>Swipr - Data Collection (Part 2 of 2)</a></li><li><a href=/posts/ml/swipr03/>Swipr - Data Collection (Part 1 of 2)</a></li><li><a href=/posts/ml/swipr02/>Swipr - Scope</a></li><li><a href=/posts/ml/swipr01/>Swipr - Overview</a></li></ul></div><div class=post__footer><span><a class=category href=/categories/machine-learning/>machine learning</a><a class=category href=/categories/fast%2eai/>fast.ai</a></span>
<span><a class=tag href=/tags/cnn/>CNN</a><a class=tag href=/tags/web-scraping/>web scraping</a><a class=tag href=/tags/data-collection/>data collection</a><a class=tag href=/tags/architecture/>architecture</a></span></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>