<!doctype html><html dir=ltr lang=en data-theme class="html theme--light"><head><title>|
Swipr - Data Cleaning</title><meta charset=utf-8><meta name=generator content="Hugo 0.109.0"><meta name=viewport content="width=device-width,initial-scale=1,viewport-fit=cover"><meta name=author content><meta name=description content="Part 5 of the Swipr series, where we discuss using DFace and InsightFace to detect faces in our dataset."><link rel=stylesheet href=/scss/main.min.5794be192d21535bdd301561e043a96b6adbad2b5c08279deff459e4661c613f.css integrity="sha256-V5S+GS0hU1vdMBVh4EOpa2rbrStcCCed7/RZ5GYcYT8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/css/markupHighlight.min.31b0a1f317f55c529a460897848c97436bb138b19c399b37de70d463a8bf6ed5.css integrity="sha256-MbCh8xf1XFKaRgiXhIyXQ2uxOLGcOZs33nDUY6i/btU=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/fontawesome.min.7d272de35b410fb165377550cdf9c4d3a80fbbcc961e111914e4d5c0eaf5729f.css integrity="sha256-fSct41tBD7FlN3VQzfnE06gPu8yWHhEZFOTVwOr1cp8=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/solid.min.55d8333481b07a08e07cf6f37319753a2b47e99f4c395394c5747b48b495aa9b.css integrity="sha256-VdgzNIGwegjgfPbzcxl1OitH6Z9MOVOUxXR7SLSVqps=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/regular.min.a7448d02590b43449364b6b5922ed9af5410abb4de4238412a830316dedb850b.css integrity="sha256-p0SNAlkLQ0STZLa1ki7Zr1QQq7TeQjhBKoMDFt7bhQs=" crossorigin=anonymous type=text/css><link rel=stylesheet href=/fontawesome/css/brands.min.9ed75a5d670c953fe4df935937674b4646f92674367e9e66eb995bb04e821647.css integrity="sha256-ntdaXWcMlT/k35NZN2dLRkb5JnQ2fp5m65lbsE6CFkc=" crossorigin=anonymous type=text/css><link rel="shortcut icon" href=/favicon.ico type=image/x-icon><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=canonical href=https://0xnf.github.io/posts/ml/swipr05/><script type=text/javascript src=/js/anatole-header.min.f9132794301a01ff16550ed66763482bd848f62243d278f5e550229a158bfd32.js integrity="sha256-+RMnlDAaAf8WVQ7WZ2NIK9hI9iJD0nj15VAimhWL/TI=" crossorigin=anonymous></script>
<script type=text/javascript src=/js/anatole-theme-switcher.min.d6d329d93844b162e8bed1e915619625ca91687952177552b9b3e211014a2957.js integrity="sha256-1tMp2ThEsWLovtHpFWGWJcqRaHlSF3VSubPiEQFKKVc=" crossorigin=anonymous></script><meta name=twitter:card content="summary"><meta name=twitter:title content="Swipr - Data Cleaning"><meta name=twitter:description content="Part 5 of the Swipr series, where we discuss using DFace and InsightFace to detect faces in our dataset."><meta property="og:title" content="Swipr - Data Cleaning"><meta property="og:description" content="Part 5 of the Swipr series, where we discuss using DFace and InsightFace to detect faces in our dataset."><meta property="og:type" content="article"><meta property="og:url" content="https://0xnf.github.io/posts/ml/swipr05/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2018-12-11T15:29:37-08:00"><meta property="article:modified_time" content="2018-12-11T15:29:37-08:00"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr01/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr02/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr03/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr04/"><meta property="og:see_also" content="https://0xnf.github.io/posts/ml/swipr06/"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","articleSection":"posts","name":"Swipr - Data Cleaning","headline":"Swipr - Data Cleaning","alternativeHeadline":"","description":"
      Part 5 of the Swipr series, where we discuss using DFace and InsightFace to detect faces in our dataset.


    ","inLanguage":"en-us","isFamilyFriendly":"true","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/0xnf.github.io\/posts\/ml\/swipr05\/"},"author":{"@type":"Person","name":""},"creator":{"@type":"Person","name":""},"accountablePerson":{"@type":"Person","name":""},"copyrightHolder":{"@type":"Person","name":""},"copyrightYear":"2018","dateCreated":"2018-12-11T15:29:37.00Z","datePublished":"2018-12-11T15:29:37.00Z","dateModified":"2018-12-11T15:29:37.00Z","publisher":{"@type":"Organization","name":null,"url":"https://0xnf.github.io/","logo":{"@type":"ImageObject","url":"https:\/\/0xnf.github.io\/favicon-32x32.png","width":"32","height":"32"}},"image":[],"url":"https:\/\/0xnf.github.io\/posts\/ml\/swipr05\/","wordCount":"1277","genre":["machine learning","fast.ai"],"keywords":["CNN","web scraping","data collection","architecture"]}</script></head><body class=body><div class=wrapper><aside class=wrapper__sidebar><div class="sidebar
animated fadeInDown"><div class=sidebar__content><div class=sidebar__introduction><img class=sidebar__introduction-profileimage src=/ alt="profile picture"><div class=sidebar__introduction-title><a href=/></a></div><div class=sidebar__introduction-description><p></p></div></div><ul class=sidebar__list></ul></div><footer class="footer footer__sidebar"><ul class=footer__list><li class=footer__item>&copy;
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></div></aside><main class=wrapper__main><header class=header><div class="animated fadeInDown"><a role=button class=navbar-burger data-target=navMenu aria-label=menu aria-expanded=false><span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span>
<span aria-hidden=true class=navbar-burger__line></span></a><nav class=nav><ul class=nav__list id=navMenu></ul><ul class="nav__list nav__list--end"><li class=nav__list-item><div class=themeswitch><a title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></li></ul></nav></div></header><div class="post
animated fadeInDown"><div class=post__content><h1>Swipr - Data Cleaning</h1><p>Back in <a href=/posts/ml/swipr03>part 3</a>, we said:</p><pre><code>Of course, pictures of beaches, food, and dogs are all common Instagram subjects, even on profiles of stereotypically self centered selfie-obsessed young twenties girls. We will come back to this point in detail later on, but for now, let's assume that each profile is pure and ideal for its category.
</code></pre><p>Now it is time to come back to that.</p><p>A bunch of Instagram photos, no matter from whose profile, are of course not pure and ideal for their category, and pictures of beaches, foods, and boyfriends abound.</p><p>This is actually a similar problem to what we might find on Tinder - people will post far more than just their picture there, often the same sets of non-face images will crop up, so this is a problem that is double worth solving.</p><p>This is just a data massaging step before we train our model, but it&rsquo;s an important one.</p><p>We&rsquo;re going to take all our images and run them through a face/gender/age summarizing model, and use that information to determine what categories (Like, Dislike) to load each image in as during model-training time.</p><pre><code>An important note about this pre-filter is that we are only looking to disqualify images. The goal of this step is explicitly to not decide what is an 'OK' image. We are only concerned with what qualifies as a 'NO' image.  
</code></pre><h2 id=a-new-database>A New Database</h2><p>Before we talk about the actual implementation of this pre-filter, we need to talk about some infrastructure.</p><p>One of the themes of this project thus far has been those of being &ldquo;robust-to-failure&rdquo; and &ldquo;easy-to-recover from&rdquo;. We take the same approach here for the pre-filter, simply because we have to go over really a lot of data. As an added bonus, the resulting database we make will be useful in constructing the dataloader for our final model training as well.</p><p>We&rsquo;ll be creating a database with one table constructed like so:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-SQL data-lang=SQL><span style=display:flex><span><span style=color:#66d9ef>CREATE</span> <span style=color:#66d9ef>TABLE</span> <span style=color:#f92672>`</span>pics<span style=color:#f92672>`</span> (
</span></span><span style=display:flex><span>	<span style=color:#f92672>`</span>path<span style=color:#f92672>`</span>          TEXT <span style=color:#66d9ef>NOT</span> <span style=color:#66d9ef>NULL</span>,
</span></span><span style=display:flex><span>	<span style=color:#f92672>`</span>label<span style=color:#f92672>`</span>         INTEGER <span style=color:#66d9ef>NOT</span> <span style=color:#66d9ef>NULL</span> <span style=color:#66d9ef>DEFAULT</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>	<span style=color:#f92672>`</span>invalid<span style=color:#f92672>`</span>       INTEGER <span style=color:#66d9ef>NOT</span> <span style=color:#66d9ef>NULL</span> <span style=color:#66d9ef>DEFAULT</span> <span style=color:#ae81ff>0</span>,
</span></span><span style=display:flex><span>	<span style=color:#f92672>`</span>ages<span style=color:#f92672>`</span>          TEXT,
</span></span><span style=display:flex><span>	<span style=color:#f92672>`</span>num_people<span style=color:#f92672>`</span>    INTEGER,
</span></span><span style=display:flex><span>	<span style=color:#f92672>`</span>male_present<span style=color:#f92672>`</span>  INTEGER,
</span></span><span style=display:flex><span>	<span style=color:#f92672>`</span>baby_present<span style=color:#f92672>`</span>  INTEGER,
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>PRIMARY</span> <span style=color:#66d9ef>KEY</span>(<span style=color:#f92672>`</span>path<span style=color:#f92672>`</span>)
</span></span><span style=display:flex><span>);
</span></span></code></pre></div><p>You can see the details of how we create this database over at the <a href=https://github.com/0xNF/swipr_gh/blob/master/InitialSorter/sort.py>sort.py</a></p><p>The purpose of this database is to keep track of various bits of information that we will make use of during model training.</p><p>The most salient bits are the <code>label</code>, <code>num_people</code> and <code>male_present</code> fields. The others are relics of experimentations that didn&rsquo;t work for one reason or another.</p><h3 id=label-primer>Label Primer</h3><p>The most important of the database fields is the label. A quick glossary of the label meanings is as follows:</p><table><thead><tr><th>Label</th><th>int</th><th>Description</th></tr></thead><tbody><tr><td>NOJUDGEMENT_INT</td><td>-1</td><td>This image has been seen, but no judgement has been made</td></tr><tr><td>UNJUDGED_INT</td><td>-2</td><td>This image has not been seen</td></tr><tr><td>FAIL_AGE_INT</td><td>-3</td><td>Someone in this image is too old or too young</td></tr><tr><td>FAIL_MALE_INT</td><td>-4</td><td>There is a male in this image</td></tr><tr><td>FAIL_NOFACE_INT</td><td>-5</td><td>This image does not contain any faces (pass 1)</td></tr><tr><td>FAIL_INVALID_INT</td><td>-6</td><td>This image failed to load properly</td></tr><tr><td>FAIL_GROUP</td><td>-7</td><td>There are too many people in this image</td></tr><tr><td>FAIL_UNCHANGED_INT  </td><td>-8   </td><td>This image does not contain any faces (pass 2)</td></tr><tr><td>FAIL_BASE_INT</td><td> 0</td><td>Unusued</td></tr></tbody></table><p>All images found by <code>sort.py</code> start out at <code>-2</code>, image not seen.</p><p>We will use these status codes to crawl through the list of images as we classify and reclassify then during our pre-filter.</p><h1 id=face-checking-round-1---dface-and-insightface>Face Checking Round 1 - DFace and InsightFace</h1><p>For the complete implementation check out the <a href=https://github.com/0xNF/swipr_gh/blob/master/ScanForFaces/ScanForFaces.ipynb>jupyter notebook</a> where the raw work was done.</p><p>We&rsquo;re going to use a pre-trained pytorch model called <a href=https://github.com/kuaikuaikim/DFace>DFace</a> to scan each picture in our dataset for faces. For each face detected by DFace, we use numpy to extract the pixels within the bounding box for that face, and then we&rsquo;ll hand that extracted face to a second model called <a href=https://github.com/deepinsight/insightface>InsightFace</a>, which performs gender and age analysis.</p><p>We load the image with cv2 and send it to DFace, which returns a list of bounding boxes, one for each face found</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>img <span style=color:#f92672>=</span> cv2<span style=color:#f92672>.</span>imread(imgpath)
</span></span><span style=display:flex><span>bboxs, landmarks <span style=color:#f92672>=</span> facedetector<span style=color:#f92672>.</span>detect_face(img)
</span></span></code></pre></div><p>then for each face found, we extract the pixels from the image given the bounding box:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>faces <span style=color:#f92672>=</span> [extract_bbox(img, x) <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> bboxs]
</span></span></code></pre></div><p>And finally we run each extracted face through <code>InsightFace's</code> <code>get_ga()</code> method:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> face <span style=color:#f92672>in</span> faces:
</span></span><span style=display:flex><span>    gd <span style=color:#f92672>=</span> genderdetector<span style=color:#f92672>.</span>get_ga(face)
</span></span></code></pre></div><p>After going through a few other custom methods we end up with an output that looks kind of like this:</p><p><img src=/ml/swipr/ga.PNG alt="pre-filtered output"></p><p>This image should also explain why we abandoned using age as a useful field to filter on earlier. The idea was to filter out toddlers and babies, but depending on the photograph, it can be horribly incorrect. Unfortunately for our dataset, it fails most egregiously on Japanese faces in particular.</p><p>The most important thing about this image is that it receives an output label of <code>-1</code>, meaning that it passed the pre-filter without problems.</p><p>Recall that the goal of this step is to filter out pictures for being bad, not determine the pictures that are good. <code>-1</code> means that it isn&rsquo;t obviously a bad picture.</p><p>(although in this particular case, it would have been nice to have this classified as <code>purikura</code> and therefore disqualified. A project for another time, which we may be able to tackle at a later date because we did classify a bunch of Instagram profiles as being primarily purikura&mldr;)</p><h1 id=face-checking-round-2---rotations>Face Checking Round 2 - Rotations</h1><p>After spending many hours watching the GPU churn checking for faces in all these images, it became obvious that there were thousands upon thousands of easily missed images where a face is clearly present but no face was detected. The sheer number of false negatives rendered a solid half of the dataset invalid.</p><p>For example:</p><p><img src=/ml/swipr/tilted.jpg alt="Tilted head"></p><p>This isn&rsquo;t even the worst example. Girls with heads tilted at even a 10 degree angle would fail to be detected.</p><p>After a bunch of experiments, we settled on a method that took each picture without a detected face (<code>label == -5</code>), ran it through the whole process again but rotated by 10 degrees, and checked it for faces again. We repeated this process until either a face was found, or we completed a full 360 degree rotation.</p><p>As one might suspect, this drastically increased the computation time for each image, up to a worst case scenario of 3600% per image if there really was no face in it. And obviously there are a lot of images with no faces in them. No matter how one rotates a picture of a sunset, it will (hopefully) never contain a human face.</p><p>The source of the rotation finder can be found in the notebook linked above, but for convenience we post it here as well:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>findrot</span>(img, already_rotated, d, verbose<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>        bboxs, _ <span style=color:#f92672>=</span> facedetector<span style=color:#f92672>.</span>detect_face(img)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> len(bboxs):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> bboxs, <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>while</span> already_rotated <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>360</span> <span style=color:#f92672>and</span> already_rotated <span style=color:#f92672>&gt;</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>360</span> <span style=color:#f92672>and</span> (d <span style=color:#f92672>!=</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>and</span> d<span style=color:#f92672>!=</span><span style=color:#ae81ff>360</span>):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> verbose: print(<span style=color:#e6db74>&#34;rotating d degrees: </span><span style=color:#e6db74>{0}</span><span style=color:#e6db74>&#34;</span><span style=color:#f92672>.</span>format(already_rotated))
</span></span><span style=display:flex><span>                img <span style=color:#f92672>=</span> rot(img, d)
</span></span><span style=display:flex><span>                already_rotated <span style=color:#f92672>+=</span> d
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>try</span>:
</span></span><span style=display:flex><span>                    bboxs, _ <span style=color:#f92672>=</span> facedetector<span style=color:#f92672>.</span>detect_face(img)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> len(bboxs) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>return</span> bboxs, already_rotated
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>except</span>:
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> verbose: print(<span style=color:#e6db74>&#34;lol sup&#34;</span>)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>continue</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> [], already_rotated
</span></span></code></pre></div><p>For any image that was completed the rotation and still came up faceless, we return a label of <code>-8</code>, indicating that it has undergone rotation and still failed.</p><p>Our face scanning architecture can be summarized with the following chart:</p><p><img src=/ml/swipr/FindRot.png alt="Find Rot"></p><p>This process took another ~18 hours on the standard issue Paperspace machine to complete. It is a very slow process, but this technique reduced the number of so called &ldquo;faceless&rdquo; images in our dataset by an impressive near 50%.</p><h1 id=moving-on>Moving On</h1><p>In the next post, we&rsquo;ll discuss using fast.ai and ResNext50 to start the training of our actual final model.</p><p><a href=/posts/ml/swipr06>Next Section - Model Architecture</a></p><h3>Posts in this series</h3><ul><li><a href=/posts/ml/swipr10/>Swipr - Server</a></li><li><a href=/posts/ml/swipr09/>Swipr - Datastore</a></li><li><a href=/posts/ml/swipr08/>Swipr - LibSwipr</a></li><li><a href=/posts/ml/swipr07/>Swipr - Swipr Script Service</a></li><li><a href=/posts/ml/swipr06/>Swipr - Fast.ai and CNNs</a></li><li><a href=/posts/ml/swipr05/>Swipr - Data Cleaning</a></li><li><a href=/posts/ml/swipr04/>Swipr - Data Collection (Part 2 of 2)</a></li><li><a href=/posts/ml/swipr03/>Swipr - Data Collection (Part 1 of 2)</a></li><li><a href=/posts/ml/swipr02/>Swipr - Scope</a></li><li><a href=/posts/ml/swipr01/>Swipr - Overview</a></li></ul></div><div class=post__footer><span><a class=category href=/categories/machine-learning/>machine learning</a><a class=category href=/categories/fast%2eai/>fast.ai</a></span>
<span><a class=tag href=/tags/cnn/>CNN</a><a class=tag href=/tags/web-scraping/>web scraping</a><a class=tag href=/tags/data-collection/>data collection</a><a class=tag href=/tags/architecture/>architecture</a></span></div></div></main></div><footer class="footer footer__base"><ul class=footer__list><li class=footer__item>&copy;
2023</li></ul></footer><script type=text/javascript src=/js/medium-zoom.min.1248fa75275e5ef0cbef27e8c1e27dc507c445ae3a2c7d2ed0be0809555dac64.js integrity="sha256-Ekj6dSdeXvDL7yfoweJ9xQfERa46LH0u0L4ICVVdrGQ=" crossorigin=anonymous></script></body></html>