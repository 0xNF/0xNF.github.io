<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Nothing in Particular</title>
    <link>https://0xnf.github.io/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Nothing in Particular</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Dec 2018 17:01:51 -0800</lastBuildDate>
    
	<atom:link href="https://0xnf.github.io/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Swipr - Server</title>
      <link>https://0xnf.github.io/posts/ml/swipr10/</link>
      <pubDate>Thu, 13 Dec 2018 17:01:51 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr10/</guid>
      <description>Recall our arch diagram from the two chapters ago:
The Database Service is a collection of methods opening and closing SQLite connections and, while important, is really exceedingly boring and furthermore, unsurprising. As such, we&amp;rsquo;ll leave it out of the discussion.
We&amp;rsquo;ll also leave out building the Razor Pages and the views, dealing with identity, migrating our initial user tables with EF Core and other such minutiae of running an ASP.</description>
    </item>
    
    <item>
      <title>Swipr - Datastore</title>
      <link>https://0xnf.github.io/posts/ml/swipr09/</link>
      <pubDate>Thu, 13 Dec 2018 15:37:02 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr09/</guid>
      <description>We need a datastore. Primarily so that we can stow a given user&amp;rsquo;s Tinder information &amp;ndash; Facebooked ID, Facebooked password, Facebooked access token, and Tinder access token, &amp;ndash; but also because we now have a complicated application it&amp;rsquo;s state must be preserved somehow.
We&amp;rsquo;re going to skip talking about tables relating to how ASP.NET Core projects do identity because there are a lot of tables for that. The only thing worth pointing out with respect to that is many of our tables will keep a foreign key to the autogenerated AspNetUsers.</description>
    </item>
    
    <item>
      <title>Swipr - LibSwipr</title>
      <link>https://0xnf.github.io/posts/ml/swipr08/</link>
      <pubDate>Thu, 13 Dec 2018 06:36:13 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr08/</guid>
      <description>As a harness for our model, we&amp;rsquo;ll create a server that serves as a centralized access point. This server provides the necessary facilities to register an account with it, interact with Tinder, and receive responses from the model.
The general architecture for the server will go something like this:
SwiprServer is the server, in our case, an ASP.NET Core 2 project, and LibSwipr is our custom tooling around all the other interactions.</description>
    </item>
    
    <item>
      <title>Swipr - Swipr Script Service</title>
      <link>https://0xnf.github.io/posts/ml/swipr07/</link>
      <pubDate>Wed, 12 Dec 2018 12:25:48 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr07/</guid>
      <description>Now that we have a trained model, we need to consider how to interact with it.
The full tech of the script can be found here on GitHub but we&amp;rsquo;ll walk through it here too.
The overview of this step is that we have a python script acting as a local server where the client side of the script receives input and returns output to to external consumers, and the server side loads the PyTorch model and runs the computation.</description>
    </item>
    
    <item>
      <title>Swipr - Fast.ai and CNNs</title>
      <link>https://0xnf.github.io/posts/ml/swipr06/</link>
      <pubDate>Tue, 11 Dec 2018 19:27:54 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr06/</guid>
      <description>Finally we can consider creating the image classification model. The jupyter notebook that was used for this process can be found here on GitHub.
DataLoader The first thing the creation of a PyTorch model requires is the creation of a DataLoader, which is a neat structure for handling the data we want to go over and what their labels should be. It loads x&amp;rsquo;s (inputs) and maps them to y&amp;rsquo;s (outputs).</description>
    </item>
    
    <item>
      <title>Swipr - Data Cleaning</title>
      <link>https://0xnf.github.io/posts/ml/swipr05/</link>
      <pubDate>Tue, 11 Dec 2018 15:29:37 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr05/</guid>
      <description>Back in part 3, we said:
Of course, pictures of beaches, food, and dogs are all common Instagram subjects, even on profiles of stereotypically self centered selfie-obsessed young twenties girls. We will come back to this point in detail later on, but for now, let&#39;s assume that each profile is pure and ideal for its category.  Now it is time to come back to that.
A bunch of Instagram photos, no matter from whose profile, are of course not pure and ideal for their category, and pictures of beaches, foods, and boyfriends abound.</description>
    </item>
    
    <item>
      <title>Swipr - Data Collection (Part 2 of 2)</title>
      <link>https://0xnf.github.io/posts/ml/swipr04/</link>
      <pubDate>Tue, 11 Dec 2018 13:29:43 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr04/</guid>
      <description>Downloading from Instagram There are a number of solutions for scraping Instagram, and it&amp;rsquo;s mostly a pick-your-poison kind of affair. we picked the first well-supported google result for &amp;ldquo;Instagram scaper&amp;rdquo; and came up with Instalooter. It seemed to come with good documentation and sufficient automation facilities.
Rate Limits We are told that prior to April 2018, downloading from Instagram was a much more lackadaisical affair. There was allegedly a generous rate limit for a given Instagram login token, upwards of 5,000 page requests per hour.</description>
    </item>
    
    <item>
      <title>Swipr - Data Collection (Part 1 of 2)</title>
      <link>https://0xnf.github.io/posts/ml/swipr03/</link>
      <pubDate>Tue, 11 Dec 2018 12:29:43 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr03/</guid>
      <description>Data Collection In previous posts we laid out what we expect our end result to look like, but underlying all of that is of course the ML model making our auto-tinder more than just a simple always-swipe-right bot.
Like all deep learning projects, before we can consider training our model or even what our model architecture will be, we need to consider the problem of data. What, exactly do we need to collect?</description>
    </item>
    
    <item>
      <title>Swipr - Scope</title>
      <link>https://0xnf.github.io/posts/ml/swipr02/</link>
      <pubDate>Tue, 11 Dec 2018 11:04:42 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr02/</guid>
      <description>From the outset we wanted to have, at the most basic level, the ability to use a CPU-only machine to judge whether or not we should swipe right on a given photograph.
Additionally, it does us no good to just have a model that does this, it needs to be useful in some form beyond merely existing, meaning it has to exist as some kind of accessible application.
Finally, it would be preferable to have this system be accessible from anywhere.</description>
    </item>
    
    <item>
      <title>Swipr - Overview</title>
      <link>https://0xnf.github.io/posts/ml/swipr01/</link>
      <pubDate>Tue, 27 Nov 2018 17:41:38 -0800</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/swipr01/</guid>
      <description>The Goal Continuing the quest to learn modern Machine Learning, I thought it would be fun to create a Tinder auto-swiper that could potentially reject matches I don&amp;rsquo;t want and accept matches I do want.
One of my friends had the brilliant idea of naming this tinderbot &amp;ldquo;Swiper&amp;rdquo;, like the fox from Dora the Explorer, famous for being repelled by the intrepid protagonists with the incantation of &amp;ldquo;Swiper, no swiping!</description>
    </item>
    
    <item>
      <title>Machine Levine Mk. II</title>
      <link>https://0xnf.github.io/posts/ml/mk2/</link>
      <pubDate>Sat, 03 Nov 2018 07:22:07 -0700</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/mk2/</guid>
      <description>Machine Levine Mk. II is the second of the Matt Bots, and the first of which can produce any kind of coherent output. A brief description of his capabilities are described at his homepage over at http://machinelevine.winetech.com/bots/ml2, but we&amp;rsquo;ll deal with some deeper ideas behind his creation and operation here.
Data Collection Mk. II uses the same initial corpus that Mk. I uses. For more details on data collection, refer to Mk.</description>
    </item>
    
    <item>
      <title>Machine Levine Mk. I</title>
      <link>https://0xnf.github.io/posts/ml/mk1/</link>
      <pubDate>Fri, 02 Nov 2018 20:08:56 -0700</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/mk1/</guid>
      <description>This is a description of how the first of the Matt Bots came into existence. Few things in this document should be interpreted as the best or even a correct way to do anything - this bot is an academic exercise. That being said, here is how he was made.
Data Collection Like the general post describes, the data used to train the Matt model was derived from Matt Levine&amp;rsquo;s at-the-time 571-count archive of Money Stuff articles.</description>
    </item>
    
    <item>
      <title>Machine Levine</title>
      <link>https://0xnf.github.io/posts/ml/machinelevine/</link>
      <pubDate>Thu, 01 Nov 2018 19:08:56 -0700</pubDate>
      
      <guid>https://0xnf.github.io/posts/ml/machinelevine/</guid>
      <description>The Goal In order to better understand machine learning, I decided to see if I could get a neural network to write articles like Matt Levine. The original goal was to sort Spotify songs by male or female vocals, but I had been learning all of this stuff by following along with the fast.ai courses, and that specific use case was a bit afield of the coursework and forum discussions happening in the MOOC.</description>
    </item>
    
  </channel>
</rss>